# Backend Architecture Deep Dive

> Repository root: `backend/`
> Application entrypoint: `backend/app/main.py`
> Frameworks: FastAPI, SQLAlchemy, Celery, Redis, ChromaDB

---

## 1. Architectural Overview
1. The backend follows a clean-architecture-inspired layout separating API routers (`app/api/v1`), domain logic (`app/domain`), core infrastructure (`app/core`), and integrations (`app/infrastructure`).
2. `main.py` constructs the FastAPI application, attaches middleware, registers routers conditionally, and initializes monitoring hooks.
3. Dependency injection relies on Python callables such as `get_db()` and `get_settings()` to provide SQLAlchemy sessions and configuration context to routers and services.
4. Authentication is handled with JWT tokens, refresh tokens, and session management across `app/core/token_manager.py`, `app/core/middleware.py`, and `app/api/v1/auth_router.py`.
5. Retrieval-Augmented Generation (RAG) capabilities span `app/domain/ai` with `EnhancedRAGService` integrating ChromaDB for vector search, and `ActionEngine` orchestrating follow-up actions.
6. Background processing uses Celery workers defined in `app/infrastructure/queue`, enabling asynchronous AI tasks, marketing sequences, and report generation.
7. Data persistence blends raw SQL via SQLAlchemy `text()` queries with Postgres tables defined in `backend/schema/database.sql` and seeded by dataset scripts.
8. Observability hooks record structured logs, metrics, and health signals for Prometheus/Grafana dashboards in `monitoring/`.

---

## 2. Directory Map (Backend)
1. `backend/app/main.py` — FastAPI application creation, middleware registration, router inclusion, and lifespan events.
2. `backend/app/api/v1` — Module-per-feature routers exposing REST/WebSocket endpoints.
3. `backend/app/core` — Cross-cutting concerns: settings, env loading, database session factory, middleware, authentication utilities, rate limiting, role enforcement.
4. `backend/app/domain` — Domain services and business logic for AI, marketing, workflows, sessions, listings, and feedback.
5. `backend/app/infrastructure` — Integration adapters for cache, database, queue, external services, and monitoring.
6. `backend/app/tests` — Test scaffolding (minimal) colocated with app.
7. `backend/auth` — Supplemental auth stack (legacy or alternate entry point) mirroring core modules.
8. `backend/config` — Additional settings overrides.
9. `backend/ml` — Machine learning utilities, analytics, embeddings, and classifiers.
10. `backend/migrations` — Database migration scripts (ensure alignment with Alembic when introduced).
11. `backend/models_legacy` — Legacy ORM models retained for backward compatibility.
12. `backend/performance` — Performance profiling scripts.
13. `backend/quality` — Quality assurance tooling.
14. `backend/routers` — Legacy routers or transitional modules.
15. `backend/schema` — Database schema definitions (`database.sql`).
16. `backend/scripts` — Backend-specific helper scripts.
17. `backend/security` — Security scanners and enforcement utilities.

---
## 3. Request Lifecycle
1. Incoming HTTP request hits FastAPI instance created in `main.py`.
2. `RequestLoggingMiddleware` logs request metadata (method, path, user) and response metrics to `logs/app.log` with configured `LOG_LEVEL`.
3. CORS middleware is configured based on `ALLOWED_ORIGINS` from `app/core/settings.py`, ensuring cross-domain compatibility with web/mobile clients.
4. Authentication dependency `get_current_user` extracts bearer token, validates via `verify_jwt_token`, and attaches user context to request state.
5. RBAC enforcement occurs through `require_roles` decorator or dependency injection, referencing role definitions in `app/core/role_based_access.py`.
6. Route handler executes, commonly injecting `Session` from `get_db()` and reading config via `get_settings()`.
7. Database operations leverage SQLAlchemy engines constructed in `app/core/database.py`, primarily through parameterized SQL executed with `session.execute(text(...))`.
8. Service modules from `app/domain` or `backend/services` encapsulate heavy logic, with routers delegating to them to maintain separation of concerns.
9. Responses are validated against Pydantic response models to ensure contract compliance and documented automatically via FastAPI.
10. Errors raise `HTTPException` or custom exceptions logged by middleware for monitoring; critical errors captured for Alertmanager.

---

## 4. Authentication & Authorization Stack
1. Password hashing uses `hash_password` and `verify_password` from `app/core/utils.py`, employing Bcrypt configured via `BCRYPT_ROUNDS`.
2. Access tokens generated by `generate_access_token` include user ID, roles, and expiration claims.
3. Refresh tokens managed by `generate_refresh_token` enable session continuity; rotation strategies handled in `auth_router.py`.
4. Token verification (`verify_jwt_token`) checks signatures using `SECRET_KEY` / `JWT_SECRET`, with expiration checks enforced.
5. `session_manager.py` records session metadata, enabling audit logs and revocation when necessary.
6. `role_based_access.py` defines role hierarchies (Admin, Agent, Manager) and capability mappings.
7. `rate_limiter.py` throttles repeated login attempts (`RATE_LIMIT_LOGIN_ATTEMPTS`) and general API traffic (`RATE_LIMIT_REQUESTS_PER_MINUTE`).
8. Development bypass modules (`dev_auth_bypass.py`, `simple_dev_auth.py`) provide short-circuited auth for local testing; ensure deactivated in production.
9. Security-critical routes incorporate `require_roles` to restrict actions such as database optimization, marketing launches, or admin configuration.
10. Audit trails for auth actions should be recorded via logging and eventually persisted in dedicated tables.

---

## 5. Core Modules Detail (`app/core`)
1. `settings.py` loads environment variables using `env_loader.py`, sets defaults for database, cache, AI providers, logging, and security parameters.
2. `database.py` configures SQLAlchemy engine with connection pooling, exposes `SessionLocal`, and handles session lifecycle in `get_db()`.
3. `middleware.py` introduces request logging, user extraction, rate limiting hooks, and custom middleware for context propagation.
4. `models.py` defines shared Pydantic models and typed structures used across routers.
5. `rate_limiter.py` integrates with Redis for tracking request counts and supports sliding window strategies.
6. `role_based_access.py` centralizes permission checks for routers to prevent drift across modules.
7. `routes.py` may provide centralized router registry or legacy aggregated routes (review for refactor opportunities).
8. `session_manager.py` manages login sessions, secure cookies (if applicable), and multi-factor placeholders.
9. `token_manager.py` handles JWT creation, rotation, blacklisting, and introspection utilities.
10. `utils.py` includes helper functions for hashing, token generation, validation, and general utilities reused across services.
11. `dev_auth_bypass.py` and `simple_dev_auth.py` exist for dev convenience; ensure environment gating is in place.
12. `env_loader.py` reads `.env` and sets environment variables before settings evaluation.

---
## 6. Router Modules Breakdown (`app/api/v1`)
1. `admin_knowledge_router.py`
   - Provides CRUD for knowledge articles, FAQs, and best practice documents stored in Postgres and indexed in ChromaDB.
   - Offers import/export endpoints for bulk knowledge updates and supports tagging for contextual AI responses.
2. `admin_router.py`
   - Manages administrative setup, including tenant configuration, plan limits, and feature toggle management.
   - Integrates with `role_based_access` to enforce admin-only actions and logs changes for auditing.
3. `ai_assistant_router.py`
   - Handles synchronous AI assistant requests using `EnhancedRAGService` and Pydantic schemas for prompts/responses.
   - Supports contextual parameters (persona, language, tone) that tune AI behavior on a per-request basis.
4. `ai_request_router.py`
   - Accepts long-running AI tasks, persists request metadata, and returns task identifiers for polling.
   - Delegates execution to Celery via `ai_request_processing_service.py`, ensuring retries and status tracking.
5. `analytics_router.py`
   - Serves aggregated metrics for dashboards, combining SQL queries with cached results in Redis.
   - Includes segment filters (time range, agent, campaign) and returns data structures fit for frontend charts.
6. `auth_router.py`
   - Implements login, logout, refresh, and password management flows with comprehensive error handling.
   - Logs suspicious activity (failed attempts, locked accounts) for security monitoring.
7. `chat_sessions_router.py`
   - Supports REST endpoints for session management and WebSocket endpoints for real-time chat.
   - Maintains conversation transcripts, context windows, and integrates with voice processing when enabled.
8. `clients_router.py`
   - Offers CRUD for client entities, budgets, preferences, and tracks relationship milestones.
   - Includes advanced filtering (status, assigned agent, budget range) and pagination for large datasets.
9. `cma_reports_router.py`
   - Generates comparative market analysis (CMA) by merging property comps, market trends, and AI narratives.
   - Supports asynchronous generation with notifications when large reports are ready.
10. `database_enhancement_router.py`
    - Provides endpoints to run database optimization scripts, analyze indexes, and preview improvement plans.
    - Requires admin privileges and careful gating in production environments.
11. `data_router.py`
    - Manages ingestion of CSV/JSON files, triggers transformation scripts, and monitors ingestion status.
    - Includes validation endpoints to preview parsed data before committing to Postgres.
12. `documents_router.py`
    - Handles document uploads, metadata management, and retrieval of processed content for AI tasks.
    - Relies on `document_processing_service.py` to extract text, classify document types, and store embeddings.
13. `feedback_router.py`
    - Collects qualitative and quantitative feedback on AI interactions, routing insights to analytics pipelines.
    - Supports filtering feedback by agent, channel, or timeframe for quality reviews.
14. `file_processing_router.py`
    - Offers specialized processing tasks such as OCR, table extraction, and format conversions.
    - Uses asynchronous jobs for heavy workloads, streaming intermediate results when necessary.
15. `health_router.py`
    - Returns health diagnostics including DB connectivity, Redis availability, and version metadata.
    - Exposes readiness states used by Docker Compose healthchecks and orchestrators.
16. `human_expertise_router.py`
    - Facilitates human-in-the-loop reviews by assigning AI outputs to reviewers, capturing decisions for future model tuning.
    - Provides export functionality for audit logs and compliance reporting.
17. `marketing_automation_router.py`
    - Enables creation of marketing campaigns, defines multi-channel messaging, and schedules execution.
    - Interacts with `marketing` domain services to personalize content and track performance.
18. `ml_advanced_router.py`
    - Hosts advanced machine learning experiments, enabling internal teams to expose pilot features.
    - Supports toggling models, retrieving metrics, and sandboxing predictions separate from production.
19. `ml_insights_router.py`
    - Delivers packaged ML insights such as churn prediction, price optimization, and lead scoring.
    - Allows batching requests and streaming results when computations are heavy.
20. `ml_websocket_router.py`
    - Provides WebSocket endpoints for streaming ML outputs, progress updates, and incremental inference results.
    - Handles client subscriptions, error forwarding, and cancellation messages.
21. `nurturing_router.py`
    - Manages nurturing sequences, defining cadence, message templates, and conditional logic for leads.
    - Works closely with Celery beat scheduler to trigger steps on schedule.
22. `performance_router.py`
    - Exposes runtime metrics such as CPU load, request latency, queue depth, and job success rates.
    - Integrates with Prometheus by publishing metrics-friendly JSON data.
23. `phase3_advanced_router.py`
    - Houses future roadmap features (phase 3) behind feature flags, enabling pilot deployments.
    - Includes gating to ensure only authorized tenants access experimental endpoints.
24. `property_detection_router.py`
    - Implements property detection/classification tasks, potentially interfacing with computer vision services.
    - Supports uploading media for analysis and returning classification results with confidence scores.
25. `property_management.py`
    - Core CRUD for property records, handling attachments, amenities, and agent assignments.
    - Implements search filters, bulk updates, and integrates with AI packages for automated property enrichment.
26. `report_generation_router.py`
    - Centralizes report creation: CMA, performance summaries, compliance documents.
    - Supports asynchronous processing, file storage management, and webhook notifications for completions.
27. `search_optimization_router.py`
    - Manages search index updates, boosting strategies, and synonyms for improved search relevance.
    - Exposes tools for analytics teams to tune search ranking algorithms.
28. `social_media_router.py`
    - Schedules posts to social platforms, tracks engagement metrics, and stores content templates.
    - Integrates with marketing automation to ensure consistent messaging across channels.
29. `task_orchestration_router.py`
    - Manages AI workflow packages, enabling creation, execution, failure recovery, and audit trails.
    - Exposes endpoints for step-level inspection and manual overrides when needed.
30. `team_management_router.py`
    - Handles team structures, roles, invitations, and permission adjustments.
    - Provides reporting on team activity, capacity, and assignment load.
31. `transactions_router.py`
    - Manages transaction lifecycle, commission calculations, document tracking, and closing workflows.
    - Integrates with `transaction_history` table to maintain an immutable record of status changes.
32. `workflows_router.py`
    - Hosts general-purpose workflows (onboarding, compliance, QA) with state machine management.
    - Allows custom workflow definitions per tenant and integrates with Celery tasks for automation.

---
## 7. Domain Layer Details (`app/domain`)
1. `ai/action_engine.py` orchestrates multi-step AI workflows, translating model outputs into concrete tasks (send email, update CRM, schedule follow-up) using internal rule sets.
2. `ai/advanced_ml_service.py` provides an interface to advanced ML models, enabling experimentation with new predictive features before mainstream adoption.
3. `ai/ai_enhancements.py` refines raw AI outputs, normalizing tone, structure, and referencing retrieved knowledge snippets.
4. `ai/ai_manager.py` abstracts provider-specific APIs (Gemini, OpenAI), handles model selection, retries, throttling, and telemetry.
5. `ai/ai_processing_service.py` executes queued AI jobs, ensuring idempotency via task IDs stored in Postgres and monitoring execution times.
6. `ai/ai_request_processing_service.py` bridges HTTP requests to asynchronous pipelines, serializing context and scheduling Celery tasks.
7. `ai/analytics_service.py` aggregates AI usage statistics (prompt counts, latency, success rate) and writes to analytics tables.
8. `ai/brand_management_service.py` maintains brand voice guidelines and ensures marketing content adheres to brand tone.
9. `ai/brokerage_management_service.py` offers analytics and automation tailored for brokerage-level oversight, including compliance checks.
10. `ai/client_nurturing_service.py` orchestrates personalized nurturing sequences blending deterministic steps with AI-generated content.
11. `ai/compliance_monitoring_service.py` scans AI outputs for compliance violations, flags risky content, and triggers human review if needed.
12. `ai/developer_panel_service.py` exposes diagnostic tools for engineers, including model latency stats, error logs, and prompt replay.
13. `ai/document_processing_service.py` encapsulates document ingestion, text extraction, classification, and metadata storage.
14. `ai/dubai_data_integration_service.py` merges local regulatory and market data into AI context, ensuring localized responses.
15. `ai/entity_detection_service.py` identifies named entities (developers, neighborhoods, policies) for structuring AI responses.
16. `ai/file_storage_service.py` abstracts file persistence (local, S3, etc.), returning signed URLs for downloads.
17. `ai/human_expertise_service.py` manages human review workflows, assigning cases, tracking decisions, and feeding improvements back to AI.
18. `ai/hybrid_search_engine.py` blends keyword search, filters, and vector similarity to deliver high-relevance context snippets.
19. `ai/intelligent_data_sorter.py` organizes retrieved knowledge into hierarchical sections ready for AI prompts.
20. `ai/intelligent_processor.py` acts as a meta-processor coordinating multiple AI sub-services for complex tasks.
21. `ai/knowledge_base_service.py` manages curated knowledge sources including curated FAQs, policies, and structured datasets.
22. `ai/notification_service.py` sends notifications via email/SMS/in-app alerts triggered by AI or workflow events.
23. `ai/package_manager.py` bundles sets of actions into reusable packages for automation (e.g., onboarding sequence, compliance review).
24. `ai/property_detection_service.py` identifies property attributes from text or images, mapping to database fields.
25. `ai/query_understanding.py` performs intent classification and entity extraction to route requests appropriately.
26. `ai/rag_service.py` orchestrates retrieval-augmented generation: embedding queries, retrieving top-k results, assembling prompts, and post-processing outputs.
27. `ai/reporting_service.py` compiles AI-driven narratives for reports, injecting charts, tables, and textual analysis.
28. `ai/response_enhancer.py` post-processes AI outputs, ensuring consistent formatting, disclaimers, and actionable next steps.
29. `ai/task_orchestrator.py` coordinates step-by-step execution of AI packages, managing error handling and rollback.
30. `ai/voice_processing_service.py` handles speech-to-text, text-to-speech, and voice intent analysis.
31. `ai/workflow_automation_service.py` unifies AI-driven steps with deterministic tasks, enabling complex automation pipelines.
32. `feedback` packages interpret user feedback, categorize issues, and feed improvement loops.
33. `listings` domain modules manage listing scoring, ranking, and data enhancement based on AI insights.
34. `marketing` modules encapsulate marketing strategies, segmentation logic, and multi-channel coordination.
35. `sessions` modules track chat history, context retention policies, and conversation analytics.
36. `workflows` modules define nodes, transitions, and state machines for orchestrated processes across the platform.

---

## 8. Infrastructure Layer (`app/infrastructure`)
1. `cache` directory likely contains Redis cache helpers for storing computed responses, tokens, or configuration (review actual files for specifics).
2. `db/database_manager.py` centralizes database initialization, migrations, and seed operations.
3. `db/database_enhancement_optimizer.py` analyzes query plans, indexes, and recommends improvements to maintain performance.
4. `db/database_index_optimizer.py` focuses on index creation/removal based on usage statistics.
5. `db/database_migrations.py` provides utilities to orchestrate migrations; align with future Alembic adoption.
6. `db/init_database.py` handles initial schema creation and seeding for fresh environments.
7. `db/seed_properties.py` seeds property data from CSVs to accelerate local setup.
8. `integrations` likely includes connectors to external APIs (CRM, marketing platforms, etc.); audit contents for each integration.
9. `queue/celery_app.py` instantiates Celery application with Redis broker/backends, registering task modules.
10. `queue/async_processing.py` defines asynchronous endpoints bridging HTTP to Celery tasks.
11. `queue/batch_processor.py` executes batch jobs, handling chunking and progress tracking for large datasets.
12. `queue/nurturing_scheduler.py` sets up periodic tasks for nurturing sequences using Celery beat.
13. `queue/scheduler.py` might coordinate additional scheduled jobs beyond nurturing.
14. `queue/task_manager.py` defines Celery tasks for AI workflows, marketing, and reporting jobs, enforcing retry policies.
15. `queue/ai_commands.py` encapsulates command patterns for AI execution pipelines.
16. `queue/__init__.py` ensures tasks are auto-discovered by Celery.

---
## 9. Data Access Patterns
1. SQLAlchemy engine uses connection pooling to Postgres defined by `DATABASE_URL`; ensure `.env` matches environment.
2. Routers use `session.execute(text(...), params)` for raw SQL, maintaining clarity on executed queries.
3. Complex queries should be centralized in domain services to prevent duplication and ease optimization.
4. Transactions are managed via SQLAlchemy `Session`; ensure commit/rollback occurs appropriately, especially in try/except blocks.
5. When multiple queries required, prefer using `session.begin()` context manager for atomic operations.
6. Use parameter binding to avoid SQL injection; never format user inputs directly into query strings.
7. For heavy read operations, consider caching results in Redis via infrastructure cache layer.
8. Schema changes require updating `database.sql`, routers, and seeding scripts; plan for Alembic migrations to manage drift.
9. Data ingestion flows should validate CSV headers/types before insertion; leverage `data_router` preview endpoints.
10. Document ingestion metadata stored in Postgres should include file checksum to prevent duplicate processing.

---

## 10. Background Processing & Task Lifecycle
1. Celery worker defined in Docker Compose (`worker` service) loads tasks from `app.infrastructure.queue` modules.
2. Task queue operations rely on Redis (`REDIS_URL`) for broker/backing store.
3. `async_processing.py` provides endpoints to enqueue tasks, returning task IDs for frontend polling.
4. `batch_processor.py` chunk large jobs to avoid timeouts, tracking progress in `package_steps` table.
5. `nurturing_scheduler.py` sets periodic tasks (daily/weekly) to advance nurturing sequences; toggled via `NURTURING_SCHEDULER_ENABLED`.
6. Task retries configured via Celery; ensure idempotent operations to avoid duplication on retries.
7. Task results stored in Postgres tables (`ai_tasks`, `package_executions`, `package_steps`) enabling detailed status dashboards.
8. Monitor Celery queues through custom metrics in `performance_router.py` and future Flower integration.
9. Prioritize tasks by setting Celery queue priorities or separate queues (e.g., `ai`, `reports`, `marketing`).
10. For long-running tasks, provide streaming updates via WebSocket endpoints or status polling endpoints.

---

## 11. AI Pipeline Stages
1. Request classification: `query_understanding.py` identifies intent, which influences retrieval and response strategy.
2. Retrieval: `rag_service.py` constructs embeddings, queries ChromaDB, and assembles top-k context documents.
3. Prompt assembly: System and user prompts combined with retrieved context, brand guidelines, and guardrails.
4. Model invocation: `ai_manager.py` selects provider (Gemini or fallback) and executes API call with retries and timeout handling.
5. Post-processing: `response_enhancer.py` cleans formatting, ensures disclaimers, and structures output sections (Executive Summary, Recommendations, Next Steps).
6. Action orchestration: `action_engine.py` interprets response metadata, triggering follow-up tasks (schedule call, update CRM) via Celery tasks.
7. Logging: Each step logs telemetry (latency, tokens, success) to analytics service for monitoring.
8. Feedback: User feedback submitted via `feedback_router` attaches to chat/session entries for future tuning.
9. Compliance: `compliance_monitoring_service.py` reviews outputs for policy adherence, optionally triggering human review.
10. Continuous improvement: Feedback loops update prompt templates, retrieval weighting, and guardrails stored in configuration.

---

## 12. Security & Compliance Controls
1. Enforce `HTTPS` termination at reverse proxy (deployment detail) while application assumes secure transport.
2. Validate all incoming payloads with Pydantic models to ensure type safety.
3. Implement RBAC for every router; avoid default open access to new endpoints.
4. Sanitize logs to avoid leaking PII; review `RequestLoggingMiddleware` output format.
5. Encrypt secrets and credentials; remove defaults in production and integrate with secret managers.
6. Audit data exports (`data_router`, `feedback_router`) to ensure only authorized roles can access.
7. Regularly rotate JWT secrets and API keys; maintain runbook for rotation.
8. Rate limit high-cost endpoints (AI, document processing) to prevent abuse.
9. Add CSRF protections for future browser-based sessions if enabling cookie auth.
10. Conduct periodic vulnerability scans using `scripts/security` (if present) or external tools.

---

## 13. Observability & Monitoring
1. `monitoring/application_metrics.py` exposes metrics endpoints for Prometheus scraping; integrate with `/metrics` route when ready.
2. `monitoring/performance_monitor.py` collects runtime stats, including queue lengths and request latencies.
3. Log formatting configured in `monitoring/logging_config.py`; ensure consistent correlation IDs for tracing.
4. Integrate with Alertmanager rules defined under `monitoring/alertmanager/` to trigger notifications on SLA breaches.
5. Track AI latency in analytics service; define thresholds to flag slow provider responses.
6. For Celery, emit metrics per task (success/failure counts) to monitor reliability.
7. Maintain dashboards for API throughput, error rates, task backlog, and AI usage spikes in `monitoring/grafana/`.
8. Implement distributed tracing (OpenTelemetry) to understand request path across HTTP, Celery, and external APIs.
9. Configure log retention policies to avoid disk saturation in `logs/` directory.
10. Provide runbooks linked from alerts to `docs/handbook/platform.md` incident section.

---
## 14. Database Schema Summary (`backend/schema/database.sql`)
1. `properties` table captures property metadata (title, description, type, price, location, area, bedrooms, bathrooms) with timestamps.
2. `clients` table stores client contact info, budget ranges, preferences (JSONB), relationship status, and assigned agent.
3. `transactions` table links properties with buyers/sellers, tracks offer/final price, commission details, dates, and metadata.
4. `transaction_history` logs status transitions with change metadata for auditing.
5. `ai_tasks` records AI job metadata (task type, status, progress, retries, error messages) for asynchronous operations.
6. `package_executions` represents orchestrated AI packages (workflow bundles) with context data and timestamps.
7. `package_steps` tracks individual steps within packages, storing inputs/outputs and statuses.
8. Additional tables may be created via migrations or routers; ensure schema docs updated when new tables added.
9. Consider introducing foreign key relationships for users, agents to ensure referential integrity.
10. Add indexes aligned with query patterns (e.g., `properties(location)`, `transactions(transaction_status)`).

---

## 15. Development Workflow Checklist
1. Clone repository and ensure `.env` populated with valid credentials.
2. Install Python dependencies using `pip install -r requirements.txt` inside virtual environment or `.venv`.
3. Install Node dependencies for frontend if running integrated tests.
4. Start Postgres, Redis, and ChromaDB using Docker Compose or native services.
5. Run `scripts/setup_database.py` to initialize schema and seed data from `data/`.
6. Launch backend with `uvicorn backend.app.main:app --reload --port 8000` for local API testing.
7. Optional: start Celery worker `celery -A app.infrastructure.queue.celery_app worker --loglevel=info` and scheduler.
8. Execute tests: `pytest backend/tests` for backend coverage; integrate with root `tests/` for cross-cutting scenarios.
9. Use `http://localhost:8000/docs` for interactive API exploration and contract verification.
10. Update documentation when introducing new modules or altering request flows.

---

## 16. Testing Strategy Expansion
1. Unit tests should cover utilities in `app/core` (token generation, rate limiter behavior) and domain services.
2. Integration tests should spin up temporary Postgres/Redis (using Docker Compose) to test routers end-to-end.
3. Contract tests must validate JSON response schemas against OpenAPI definitions.
4. Celery task tests should utilize Celery test harness with in-memory broker (`CELERY_TASK_ALWAYS_EAGER=True` for unit-level checks).
5. RAG pipeline tests should assert retrieval accuracy by seeding small Chroma collections and verifying outputs.
6. Database migration tests must ensure schema drift is caught by comparing `database.sql` with live migrations.
7. Performance tests (Locust/K6) should evaluate high-traffic endpoints (auth, property search, AI assist) and Celery throughput.
8. Security tests should include penetration testing for auth endpoints and fuzzing for file upload handlers.
9. Observability tests should verify metrics exposure and alert triggers under simulated failure scenarios.
10. Continuous integration should block merges if tests fail or coverage drops below agreed thresholds.

---

## 17. Error Handling Patterns
1. Use `HTTPException` with descriptive `detail` payloads for client errors (400-series).
2. Log stack traces for server errors (500-series) with correlation IDs for easier debugging.
3. Wrap database operations in try/except blocks to rollback sessions on failure.
4. For Celery tasks, configure automatic retries for transient errors and escalate persistent failures via alerts.
5. Provide graceful fallbacks for AI providers (e.g., degrade to canned responses when providers unavailable).
6. When file processing fails, store failure reason in metadata tables and expose via `/data/status` endpoints.
7. Capture validation errors in Pydantic models and relay user-friendly messages to clients.
8. Use structured logging (JSON) for key events to support search and analytics.
9. Document error codes and messages in API docs for client team reference.
10. Ensure rate limiting responses include `Retry-After` headers where applicable.

---

## 18. Performance Considerations
1. Optimize SQL queries with indexes on frequently filtered columns (status, agent_id, location).
2. Cache heavy read responses (analytics summaries, marketing templates) in Redis with appropriate TTL.
3. Use pagination for list endpoints to prevent large payloads.
4. Stream large report downloads instead of loading fully into memory.
5. Monitor Celery task execution time; break long tasks into smaller steps to maintain responsiveness.
6. Use asynchronous file IO for large file uploads and processing when possible.
7. Offload CPU-intensive ML computations to background workers rather than blocking HTTP requests.
8. Benchmark RAG retrieval latency; adjust embedding size, top-k, and caching strategies accordingly.
9. Use connection pooling to avoid overhead of frequent database connections.
10. Add performance tests to catch regressions early.

---

## 19. Deployment Notes
1. Dockerfile (located in `backend/`) should produce lean production image; review for multi-stage build optimizations.
2. `docker-compose.yml` defines dependencies; ensure environment variables propagate correctly when deployed in staging/production.
3. Celery worker and scheduler require persistent volumes if storing logs; verify container restarts do not lose state.
4. Configure readiness/liveness probes in orchestration to rely on `/health` and optionally new `/ready` endpoint.
5. Utilize secrets management for sensitive environment variables rather than plain `.env` in production.
6. Plan for blue/green or canary deployments to minimize downtime during releases.
7. Integrate migrations into deployment pipeline (Alembic recommended) to ensure schema updates before API rollout.
8. Setup centralized logging (ELK, Datadog) to aggregate backend logs across containers.
9. For AI providers, manage rate limits via configuration and exponential backoff.
10. Document rollback procedure in case of failed deployment.

---

## 20. Module Interaction Diagram (Textual)
1. `main.py` -> includes routers -> routers depend on `app/core` utilities (`get_db`, `get_current_user`).
2. Routers -> call domain services in `app/domain` for business logic.
3. Domain services -> interact with infrastructure modules (database manager, Celery tasks, external integrations).
4. Domain services -> call AI providers via `ai_manager`, retrieving knowledge using `rag_service`.
5. Celery tasks -> run domain services asynchronously, updating Postgres tables with progress.
6. Monitoring -> collects metrics from middleware, domain services, and Celery hooks.
7. Feedback loops -> store data in Postgres, re-used by domain analytics services.
8. Shared utilities -> logging, configuration, security modules support all layers.
9. External dependencies -> Postgres, Redis, ChromaDB, AI APIs (Gemini, optional OpenAI).
10. Clients (web/mobile) -> interact via HTTP/WebSocket -> rely on consistent contracts documented via FastAPI.

---
## 21. Data Ingestion Workflows
1. Bulk datasets located in `data/` (properties, clients, transactions) feed into `data_router` ingestion endpoints.
2. `scripts/setup_database.py` uses these datasets to seed local environments; ensure consistent order to satisfy foreign key constraints.
3. Document ingestion (PDF/DOCX) relies on `document_processing_service.py` and stores embeddings in ChromaDB; metadata persisted in Postgres.
4. Data validation routines should inspect CSV schemas, convert datatypes, and handle missing values before insertion.
5. Use `data_router` preview endpoint to verify dataset before commit; highlight mismatched headers or invalid rows.
6. Data refresh operations should log start/end times, row counts, and errors to monitoring dashboards.
7. For incremental updates, design delta ingestion scripts referencing `updated_at` columns.
8. Large dataset operations should run asynchronously via Celery to avoid blocking HTTP requests.
9. Retain raw uploads in `uploads/` with naming conventions including timestamp and checksum.
10. Document ingestion pipelines should support reprocessing in case of updated parsing logic.

---

## 22. File Handling & Storage
1. Uploaded files stored in `uploads/` directory by default; ensure directory security (permissions, scanning).
2. `settings.py` defines allowed extensions and maximum file size; adjust to meet business requirements.
3. Use `secure_filename` (from Werkzeug) to sanitize filenames, preventing path traversal attacks.
4. For large deployments, migrate storage to S3 or equivalent, updating `file_storage_service.py` accordingly.
5. Track file metadata (owner, type, checksum, processing status) in Postgres for auditability.
6. Implement cleanup jobs to remove stale temporary files, using scripts in `scripts/backup` or new maintenance tasks.
7. Provide pre-signed URL support for secure download via `documents_router`.
8. Ensure file deletion respects retention policies and audit requirements.
9. For scanned documents, integrate OCR to convert to searchable text before embedding.
10. Monitor storage utilization and add alerts when exceeding thresholds.

---

## 23. External Integrations
1. AI providers: `ai_manager.py` currently supports Google Gemini; optionally integrate OpenAI using environment toggles.
2. Voice services: `voice_processing_service.py` may depend on speech APIs; document credentials and limitations.
3. CRM integrations: Review `integrations` directory for connectors to third-party CRMs (if present) and maintain API keys.
4. Social platforms: `social_media_router.py` likely interfaces with APIs (Facebook, Instagram, LinkedIn); secure tokens appropriately.
5. Email/SMS services: `notification_service.py` may use providers like SendGrid/Twilio; define configuration and error handling.
6. Analytics exports: Ensure integration with analytics platforms respects rate limits and data agreements.
7. Data providers: `dubai_data_integration_service.py` may consume government datasets; document refresh cadence and licensing constraints.
8. Monitoring: Optional integration with Sentry (see `monitoring/sentry_config.py`) for error tracking.
9. Payment or commission processing (if any) should adhere to compliance requirements; audit code paths carefully.
10. Provide fallback strategies for each external dependency to maintain service continuity.

---

## 24. Code Style & Conventions
1. Follow PEP8/Black formatting for Python code; adopt consistent linting pipeline (flake8, mypy) in future CI.
2. Utilize type hints extensively for clarity and better tooling support.
3. Keep router functions concise by delegating complex logic to domain services.
4. Use descriptive logging messages with context fields (user_id, session_id, task_id).
5. Document functions with docstrings when logic is non-trivial or requires context.
6. Organize imports using standard library -> third-party -> local modules.
7. Maintain consistent naming for asynchronous tasks (verb_noun_task) to ease discovery.
8. Provide Pydantic models for request/response payloads instead of using raw dicts.
9. Use Enum classes for choice fields (status, intent) to enforce valid values.
10. Keep configuration values centralized; avoid hardcoding values across modules.

---

## 25. Logging Strategy
1. `logging.basicConfig` in `main.py` sets format and level; consider migrating to structured logging config.
2. Use module-level loggers (`logging.getLogger(__name__)`) and include context data via `extra` parameter.
3. For heavy loops or batch operations, log progress at intervals to avoid log floods.
4. Mask sensitive data (passwords, tokens) before logging.
5. Standardize error log structure to ease parsing (fields: event, error_code, message, user_id, request_id).
6. Integrate log forwarding to centralized system (ELK, Datadog) for aggregating across services.
7. Provide correlation IDs across HTTP requests and Celery tasks to trace end-to-end flows.
8. Ensure logging configuration respects environment (debug vs production log levels).
9. Add logging to AI provider interactions to measure external latency and failure reasons.
10. Document log retention and archival policies for compliance.

---

## 26. Observability TODOs
1. Implement `/metrics` endpoint exposing Prometheus metrics from `monitoring` modules.
2. Add trace instrumentation to key functions (router handlers, domain services, Celery tasks).
3. Expand Grafana dashboards to include AI usage, dataset ingestion success rates, and marketing automation KPIs.
4. Build alert rules for AI latency spikes, task failure rates, and authentication anomalies.
5. Provide runbook references within alert annotations to speed remediation.
6. Include log-based alerts for suspicious login attempts or data export bursts.
7. Evaluate anomaly detection for conversion metrics to catch unexpected drops.
8. Add synthetic monitoring (scheduled API pings) to detect outages proactively.
9. Provide dashboards for queue length trending to anticipate scaling needs.
10. Document monitoring setup in `docs/handbook/platform.md` to ensure reproducibility.

---

## 27. Documentation Maintenance Tasks
1. Update this backend handbook whenever new routers, services, or infrastructure modules are added.
2. Include sequence diagrams for complex workflows (AI tasks, marketing sequences) as textual descriptions until visual assets produced.
3. Keep data schema section aligned with latest migrations; add new tables immediately.
4. Document breaking changes in API contracts and notify frontend/mobile teams.
5. Maintain `CHANGELOG-Frontend-Refactor.md` parity by referencing API changes where appropriate.
6. Encourage engineers to add docstrings and inline comments for complex service logic.
7. Record incident learnings in `docs/progress/dev-journal.md` and update relevant sections here.
8. Archive deprecated modules with notes before deletion to preserve context.
9. Provide onboarding checklists referencing this document to new team members.
10. Schedule quarterly documentation reviews to prevent drift.

---
## 28. Endpoint & Service Cross-Reference
1. `/auth/login` -> `auth_router.login` -> uses `get_db`, `hash_password`, `generate_access_token`.
2. `/auth/refresh` -> `auth_router.refresh_token` -> depends on `verify_jwt_token`, `generate_access_token`.
3. `/clients` (GET/POST) -> `clients_router.list_clients` / `clients_router.create_client` -> interacts with `client_nurturing_service` for post-creation automation.
4. `/clients/{client_id}` -> updates preferences -> triggers `notification_service` for assignment alerts.
5. `/properties` -> `property_management.create_property` -> may enqueue `action_engine` tasks for enrichment.
6. `/properties/search` -> uses SQL filters and optional AI ranking via `hybrid_search_engine`.
7. `/transactions` -> interacts with `transactionStore` data via SQL; updates `transaction_history`.
8. `/transactions/{id}/status` -> ensures state transitions valid using domain validation rules.
9. `/ai/assist` -> `ai_assistant_router.handle_assist` -> orchestrates RAG pipeline.
10. `/ai/assist/task` -> asynchronous path invoking Celery task `ai_processing_service.process_request`.
11. `/chat/sessions` -> `chat_sessions_router.create_session` -> logs session in Postgres.
12. `/chat/stream/{id}` -> WebSocket -> uses `EnhancedRAGService` streaming responses.
13. `/documents/upload` -> `documents_router.upload_document` -> stores file, schedules processing.
14. `/documents/{id}` -> fetch processed outputs -> uses `document_processing_service` to format response.
15. `/marketing/campaigns` -> `marketing_automation_router.create_campaign` -> persists config, schedules jobs.
16. `/marketing/campaigns/{id}/launch` -> triggers Celery tasks `nurturing_scheduler.launch_campaign`.
17. `/social/posts` -> `social_media_router.create_post` -> interacts with external APIs via integrations.
18. `/social/posts/{id}/status` -> polls status from external provider.
19. `/reports/cma` -> `report_generation_router.generate_cma_report` -> uses `reporting_service` and `ai/reporting_service`.
20. `/reports/performance` -> compiles analytics via SQL and AI commentary.
21. `/performance/runtime` -> `performance_router.get_runtime_metrics` -> aggregates metrics from monitoring modules.
22. `/performance/celery` -> fetches Celery stats via `task_manager.get_celery_stats`.
23. `/data/import` -> `data_router.import_data` -> uses CSV parser, SQL insertion, logs outcome.
24. `/data/status` -> returns ingestion history from tracking tables.
25. `/database/optimize` -> `database_enhancement_router.optimize` -> wraps infrastructure optimizer scripts.
26. `/nurturing/sequences` -> `nurturing_router.create_sequence` -> interacts with `workflow_automation_service`.
27. `/nurturing/sequences/{id}/pause` -> toggles Celery beat schedule entries.
28. `/tasks/packages` -> `task_orchestration_router.list_packages` -> queries `package_executions` table.
29. `/tasks/packages/{id}/resume` -> restarts package using `task_orchestrator.resume_package`.
30. `/workflows/templates` -> returns available workflow blueprints referencing domain templates.
31. `/workflows/{id}/state` -> details current state machine step and pending transitions.
32. `/ml/price-forecast` -> `ml_advanced_router.price_forecast` -> calls advanced ML model, caches results.
33. `/ml/risk-score` -> calculates compliance risk using `compliance_monitoring_service`.
34. `/ml/market-trends` -> aggregates analytics, AI commentary for market insights.
35. `/ml/lead-scoring` -> outputs lead scores used by frontend dashboards.
36. `/ml/stream` (WebSocket) -> streams predictions for progressive updates.
37. `/feedback` -> stores feedback, triggers `analytics_service` to update satisfaction metrics.
38. `/feedback/export` -> returns CSV compiled from feedback table.
39. `/health` -> simple status check verifying DB and Redis connectivity.
40. `/admin/knowledge` -> knowledge management endpoints for base content.
41. `/admin/settings` -> adjust platform-level settings with audit logging.
42. `/phase3/pilots` -> toggles pilot features for experimentation.
43. `/phase3/insights` -> shares beta analytics for internal stakeholders.
44. `/search/reindex` -> enqueues search reindex tasks.
45. `/search/boosting` -> updates boosting settings in search engine config.
46. `/files/text-extract` -> triggers text extraction pipeline.
47. `/files/tabular-preview` -> returns preview for data validation.
48. `/expertise/reviews` -> manages human review queue.
49. `/expertise/decisions` -> stores decisions for AI retraining.
50. `/analytics/summary` -> aggregated analytics for dashboards.

---

## 29. Celery Task Reference (Illustrative)
1. `task_manager.process_ai_task(task_id)` -> executes AI job with context from Postgres.
2. `task_manager.generate_report(package_id)` -> orchestrates multi-step report creation.
3. `task_manager.launch_campaign(campaign_id)` -> iterates through audience segments, sends messages.
4. `task_manager.refresh_embeddings(dataset_id)` -> re-embeds documents when dataset updated.
5. `task_manager.run_database_maintenance()` -> executes optimization scripts during low-traffic windows.
6. `task_manager.sync_social_post(post_id)` -> publishes to social platforms and updates status.
7. `task_manager.calculate_lead_scores()` -> processes lead data nightly to update scores.
8. `task_manager.trigger_follow_up(sequence_id, step)` -> sends follow-up messages based on nurturing plan.
9. `task_manager.retrain_model(model_id)` -> placeholder for future ML retraining pipeline.
10. `task_manager.cleanup_temp_files()` -> removes outdated files from uploads directory.
11. `task_manager.send_alert(alert_id)` -> dispatches urgent notifications to admins.
12. `task_manager.ingest_bulk_data(job_id)` -> handles large data imports chunk by chunk.
13. `task_manager.validate_document(document_id)` -> runs compliance checks on processed document.
14. `task_manager.update_search_index()` -> reindexes search engine with latest data.
15. `task_manager.generate_voice_response(session_id)` -> synthesizes voice output for AI session.

---

## 30. Future Enhancements (Backend-Specific)
1. Implement Alembic migrations and continuous schema verification.
2. Add API versioning strategy (v2) to support contract evolution.
3. Introduce GraphQL gateway for complex analytics if required by product roadmap.
4. Implement caching strategies per router based on usage analytics.
5. Add multi-tenancy support with tenant-specific database schemas or row-level security.
6. Integrate feature flagging service to enable controlled rollouts.
7. Build automated load testing pipeline to monitor performance across releases.
8. Improve developer ergonomics with CLI tools for scaffolding routers/services.
9. Implement domain event bus for decoupled inter-service communication.
10. Add service-level objectives (SLOs) and error budgets tracked via monitoring stack.

---

## 31. Reference Tables & Views To Add (Recommendations)
1. `users` table enhancements: store role, last_login, failed_attempts for security analytics.
2. `agents` table referencing team assignments and performance metrics.
3. `campaign_events` table to log marketing sends, opens, clicks, conversions.
4. `ai_feedback` table linking sessions to feedback entries for training.
5. `document_versions` table to track document reprocessing history.
6. `workflow_runs` table capturing workflow state transitions with timestamps.
7. Materialized views for analytics (e.g., `mv_monthly_transactions`, `mv_lead_conversion`).
8. `integration_tokens` table storing OAuth tokens securely (encrypted at rest).
9. `audit_logs` table capturing admin changes, data exports, and security-related events.
10. `system_settings` table for dynamic configuration without redeploys.

---

## 32. Revision Log
1. 2025-10-02 — Comprehensive backend documentation authored to match current codebase state.

---
